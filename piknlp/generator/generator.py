# piknlp/generator/generator.py

import logging
import re
import pandas as pd
import json
from pathlib import Path
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, MofNCompleteColumn, TimeElapsedColumn, TimeRemainingColumn
from sklearn.model_selection import train_test_split
from concurrent.futures import ThreadPoolExecutor, as_completed

from piknlp.common.config import Config
from piknlp.common.logger import get_logger
from piknlp.schema.generator import Review_Sentiment_Sample, Review_Category_Sample, SentimentList, CategoryList, Review_Sentiment_Label
from piknlp.generator.llm import call_ollama as call_llm
from piknlp.generator.llm import generate_dataset_prompt, generate_category_prompt, generate_dataset_batch_prompt, generate_category_batch_prompt

class Generator:
    logger: logging.Logger = get_logger(__name__)
    
    def __init__(self, config: Config) -> None:
        self.config = config

    def generate_dataset(self) -> None:
        
        # Load raw data
        raw_file_path: Path = self.config.task_dir / "raw" / self.config.raw_data_file # /data/sentiment/raw/reviews.csv
        self.logger.info(f"üöÄ Start generating dataset")
        self.logger.debug(f"Processing {raw_file_path}")
        reviews: list[str] = self._load_csv(raw_file_path)
        
        labeled_data: list[Review_Sentiment_Sample | Review_Category_Sample] = []
        failed_reviews: list[str] = []  # Ïã§Ìå®Ìïú Î¶¨Î∑∞Îì§ÏùÑ ÏàòÏßë

        # Î¶¨Î∑∞Î•º 5Í∞úÏî© Î¨∂Ïñ¥ÏÑú Î∞∞Ïπò ÏÉùÏÑ±
        batch_size = 2
        review_batches = [reviews[i:i + batch_size] for i in range(0, len(reviews), batch_size)]
        self.logger.info(f"Created {len(review_batches)} batches")
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            MofNCompleteColumn(),
            TimeElapsedColumn(),
            TimeRemainingColumn(),
        ) as progress:
            task = progress.add_task("[green]Î¶¨Î∑∞ Î∞∞Ïπò Ï≤òÎ¶¨ Ï§ë...", total=len(review_batches))
            with ThreadPoolExecutor(max_workers=self.config.num_workers) as executor:
                # Î∞∞ÏπòÏôÄ Ïù∏Îç±Ïä§Î•º Ìï®Íªò Ï†ÑÎã¨
                futures = {executor.submit(self._process_review_batch, batch): i for i, batch in enumerate(review_batches)}
                for f in as_completed(futures):
                    batch_idx = futures[f]
                    try:
                        batch_samples = f.result()
                        for sample in batch_samples:
                            if sample.label:  # Ïú†Ìö®Ìïú Î†àÏù¥Î∏îÏù¥ ÏûàÎäî Í≤ΩÏö∞Îßå Ï∂îÍ∞Ä
                                labeled_data.append(sample)
                    except Exception as e:
                        self.logger.error(f"Error processing review batch {batch_idx}: {e}")
                        # Ïã§Ìå®Ìïú Î∞∞ÏπòÏùò Î¶¨Î∑∞Îì§ÏùÑ ÏàòÏßë
                        failed_batch = review_batches[batch_idx]
                        failed_reviews.extend(failed_batch)
                    finally:
                        progress.update(task, advance=1)
        
        # Ïã§Ìå®Ìïú Î¶¨Î∑∞Îì§ÏùÑ Í∞úÎ≥Ñ Ï≤òÎ¶¨
        if failed_reviews:
            self.logger.info(f"Processing {len(failed_reviews)} failed reviews individually...")
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                MofNCompleteColumn(),
                TimeElapsedColumn(),
                TimeRemainingColumn(),
            ) as progress:
                task = progress.add_task("[yellow]Ïã§Ìå®Ìïú Î¶¨Î∑∞ Í∞úÎ≥Ñ Ï≤òÎ¶¨ Ï§ë...", total=len(failed_reviews))
                with ThreadPoolExecutor(max_workers=self.config.num_workers) as executor:
                    futures = [executor.submit(self._process_review, review) for review in failed_reviews]
                    for f in as_completed(futures):
                        try:
                            sample = f.result()
                            if sample.label:  # Ïú†Ìö®Ìïú Î†àÏù¥Î∏îÏù¥ ÏûàÎäî Í≤ΩÏö∞Îßå Ï∂îÍ∞Ä
                                labeled_data.append(sample)
                        except Exception as e:
                            self.logger.error(f"Error processing individual review: {e}")
                        finally:
                            progress.update(task, advance=1)
        
        save_path = self.config.task_dir / "dataset" / f"{self.config.task}.jsonl" # /data/sentiment/dataset/sentiment.jsonl
        self._save_json(labeled_data, save_path)
        self.logger.info("‚úÖ Dataset generated successfully.")
        self.logger.debug(f"Saved {len(labeled_data)} samples to {save_path}")

    def split_dataset(self, include_dev: bool = False) -> None:
        dataset_path = self.config.task_dir / "dataset" / f"{self.config.task}.jsonl" # /data/sentiment/dataset/sentiment.jsonl
        self.logger.info(f"üîç Splitting dataset from {dataset_path}")
        with open(dataset_path, "r", encoding="utf-8") as f:
            data = [json.loads(line) for line in f]
        self.logger.debug(f"Loaded {len(data)} samples from {dataset_path}")

        if include_dev:
            train_data, temp_data = train_test_split(
                data, test_size=(1 - self.config.split_ratio["train_ratio"]), random_state=self.config.split_ratio["seed"]
            )
            dev_size = self.config.split_ratio["dev_ratio"] / (1 - self.config.split_ratio["train_ratio"])
            dev_data, test_data = train_test_split(
                temp_data, test_size=(1 - dev_size), random_state=self.config.split_ratio["seed"]
            )
            splits = [("train", train_data), ("dev", dev_data), ("test", test_data)]
        else:
            train_data, test_data = train_test_split(
                data, test_size=(1 - self.config.split_ratio["train_ratio"]), random_state=self.config.split_ratio["seed"]
            )
            splits = [("train", train_data), ("test", test_data)]

        for name, split in splits:
            with open(self.config.task_dir / "dataset" / f"{name}.jsonl", "w", encoding="utf-8") as f: # /data/sentiment/dataset/{name}.jsonl
                for item in split:
                    json.dump(item, f, ensure_ascii=False)
                    f.write("\n")
        self.logger.info("‚úÖ Dataset split successfully.")
        self.logger.debug(f"Saved {len(train_data)} samples")

    def _load_csv(self, csv_path: Path, review_column: str = "Review") -> list[str]:
        df = pd.read_csv(csv_path)
        return df[review_column].dropna().tolist()

    def _save_json(self, data: list[dict], path: Path) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            for item in data:
                f.write(item.model_dump_json() + "\n")

    def _extract_json(self, text: str) -> str:
        try:
            # Ïó¨Îü¨ JSON Í∞ùÏ≤¥Í∞Ä ÏûàÏùÑ Ïàò ÏûàÏúºÎØÄÎ°ú Í∞ÄÏû• ÌÅ∞ JSON Î∏îÎ°ùÏùÑ Ï∞æÍ∏∞
            matches = list(re.finditer(r"\{[\s\S]*\}", text))
            if not matches:
                self.logger.warning("No JSON pattern found in response")
                return "{}"
            
            # Í∞ÄÏû• Í∏¥ JSON Î∏îÎ°ù ÏÑ†ÌÉù (Î≥¥ÌÜµ Í∞ÄÏû• ÏôÑÏ†ÑÌïú Í≤É)
            longest_match = max(matches, key=lambda m: len(m.group(0)))
            json_str = longest_match.group(0)
            
            # JSON ÌååÏã± ÏãúÎèÑ
            data = json.loads(json_str)
            
            # Î∞∞Ïπò ÏùëÎãµÏù∏ÏßÄ ÌôïÏù∏ (results ÌÇ§Í∞Ä ÏûàÎäîÏßÄ)
            if "results" in data:
                return json_str  # Î∞∞Ïπò ÏùëÎãµÏùÄ Í∑∏ÎåÄÎ°ú Î∞òÌôò
            
            # Îã®Ïùº ÏùëÎãµÏù∏ Í≤ΩÏö∞ Í∏∞Ï°¥ Î°úÏßÅ Ï†ÅÏö©
            if "categories" not in data and "sentiments" not in data:
                if self.config.task == "category":
                    data = {"categories": data}
                elif self.config.task == "sentiment":
                    data = {"sentiments": data}
            
            return json.dumps(data, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è JSON extraction failed: {e}")
            self.logger.debug(f"Text that failed to parse: {text[:500]}...")
        return "{}"

    def _parse_labels(self, response: SentimentList | CategoryList) -> dict[str, str] | list[Review_Sentiment_Label]:
        try:
            if isinstance(response, SentimentList):
                return [
                    Review_Sentiment_Label(category=cat, review=sentiment)
                    for cat, sentiment in response.sentiments.items()
                ]
            elif isinstance(response, CategoryList):
                return {cat: sub_cat for cat, sub_cat in response.categories.items()}
            else:   
                raise ValueError(f"Invalid response type: {type(response)}")
        except Exception as e:
            self.logger.error(f"Error parsing labels: {e}")
            return []
    
    def _validate_labels(self, labels: dict[str, str] | list[Review_Sentiment_Label]) -> bool:
        """ÏÉùÏÑ±Îêú Î†àÏù¥Î∏îÏù¥ Ïú†Ìö®ÌïúÏßÄ Í≤ÄÏ¶ù"""
        try:
            if self.config.task == "category":
                if not isinstance(labels, dict):
                    return False
                
                # Í∞Å Ïπ¥ÌÖåÍ≥†Î¶¨Î≥ÑÎ°ú Ïú†Ìö®Ìïú Í∞íÏù∏ÏßÄ ÌôïÏù∏
                for category, value in labels.items():
                    if category not in self.config.category:
                        self.logger.warning(f"Unknown category: {category}")
                        return False
                    
                    if value not in self.config.category[category]:
                        self.logger.warning(f"Invalid value '{value}' for category '{category}'. Valid values: {self.config.category[category]}")
                        return False
                
                return True
                
            elif self.config.task == "sentiment":
                if not isinstance(labels, list):
                    return False
                
                # Í∞êÏÑ± Î∂ÑÏÑùÏùò Í≤ΩÏö∞ 'pos', 'neg', 'none'Îßå Ïú†Ìö®
                valid_sentiments = {'pos', 'neg', 'none'}
                for label in labels:
                    if not isinstance(label, Review_Sentiment_Label):
                        return False
                    if label.review not in valid_sentiments:
                        self.logger.warning(f"Invalid sentiment value: {label.review}. Valid values: {valid_sentiments}")
                        return False
                
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"Error validating labels: {e}")
            return False
    
    def _process_review_batch(self, reviews: list[str]) -> list[Review_Sentiment_Sample | Review_Category_Sample]:
        """Î¶¨Î∑∞ Î∞∞ÏπòÎ•º Ï≤òÎ¶¨ÌïòÏó¨ Í∞úÎ≥Ñ ÏÉòÌîåÎì§ÏùÑ Î∞òÌôò"""
        if self.config.task == "category":
            prompt = generate_category_batch_prompt(reviews, self.config.category)
        elif self.config.task == "sentiment":
            prompt = generate_dataset_batch_prompt(reviews, self.config.category)
        else:
            raise ValueError(f"Invalid task: {self.config.task}")

        raw_response = call_llm(prompt)
        self.logger.debug(f"Raw batch response: {raw_response}")
        clean_response = self._extract_json(raw_response)
        self.logger.debug(f"Cleaned batch response: {clean_response}")
        
        samples = []
        try:
            # Î∞∞Ïπò ÏùëÎãµ ÌååÏã±
            if not clean_response or clean_response == "{}":
                self.logger.warning("Empty or invalid response, falling back to individual processing")
                raise ValueError("Empty response")
                
            batch_data = json.loads(clean_response)
            results = batch_data.get("results", [])
            self.logger.debug(f"Parsed results count: {len(results)}")
            
            if not results:
                self.logger.warning("No results in batch response, falling back to individual processing")
                raise ValueError("No results in response")
            
            for i, result in enumerate(results):
                if i >= len(reviews):
                    self.logger.warning(f"Result index {i} exceeds review count {len(reviews)}")
                    break
                    
                review_text = result.get("review", reviews[i])
                
                if self.config.task == "sentiment":
                    sentiments = result.get("sentiments", {})
                    if not sentiments:
                        self.logger.warning(f"No sentiments found for review {i}, skipping")
                        continue
                    labels = [
                        Review_Sentiment_Label(category=cat, review=sentiment)
                        for cat, sentiment in sentiments.items()
                    ]
                    # Î†àÏù¥Î∏î Í≤ÄÏ¶ù
                    if not self._validate_labels(labels):
                        self.logger.warning(f"Invalid labels for review {i}, skipping")
                        continue
                    sample = Review_Sentiment_Sample(sentence=review_text, label=labels)
                elif self.config.task == "category":
                    categories = result.get("categories", {})
                    if not categories:
                        self.logger.warning(f"No categories found for review {i}, skipping")
                        continue
                    # Î†àÏù¥Î∏î Í≤ÄÏ¶ù
                    if not self._validate_labels(categories):
                        self.logger.warning(f"Invalid labels for review {i}, skipping")
                        continue
                    sample = Review_Category_Sample(sentence=review_text, label=categories)
                else:
                    continue
                    
                samples.append(sample)
                
        except Exception as e:
            self.logger.error(f"Error parsing batch response: {e}")
            self.logger.debug(f"Raw response: {raw_response}")
            self.logger.debug(f"Cleaned response: {clean_response}")
            # Î∞∞Ïπò Ï≤òÎ¶¨ Ïã§Ìå® Ïãú ÏòàÏô∏Î•º Îã§Ïãú Î∞úÏÉùÏãúÏºúÏÑú ÏÉÅÏúÑÏóêÏÑú Ïã§Ìå®Ìïú Î¶¨Î∑∞Îì§ÏùÑ ÏàòÏßëÌïòÎèÑÎ°ù Ìï®
            raise Exception(f"Batch processing failed: {e}")
        
        return samples

    def _process_review(self, review: str) -> Review_Sentiment_Sample | Review_Category_Sample:
        if self.config.task == "category":
            prompt = generate_category_prompt(review, self.config.category)
            response_class = CategoryList
        elif self.config.task == "sentiment":
            prompt = generate_dataset_prompt(review, self.config.category)
            response_class = SentimentList
        else:
            raise ValueError(f"Invalid task: {self.config.task}")

        raw_response = call_llm(prompt)
        clean_response = self._extract_json(raw_response)
        try:
            response_obj = response_class.model_validate_json(clean_response)
            labels = self._parse_labels(response_obj)
            
            # Î†àÏù¥Î∏î Í≤ÄÏ¶ù
            if not self._validate_labels(labels):
                self.logger.warning(f"Invalid labels for review, returning empty labels")
                labels = [] if self.config.task == "sentiment" else {}
                
        except Exception as e:
            self.logger.error(f"Error parsing labels: {e}")
            self.logger.debug(f"Raw response: {raw_response}")
            self.logger.debug(f"Cleaned response: {clean_response}")
            labels = [] if self.config.task == "sentiment" else {}

        if self.config.task == "sentiment":
            return Review_Sentiment_Sample(sentence=review, label=labels)
        elif self.config.task == "category":
            return Review_Category_Sample(sentence=review, label=labels)
        else:
            raise ValueError(f"Invalid task: {self.config.task}")